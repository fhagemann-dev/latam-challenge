{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b95b05-aeec-4b36-ae91-c65b9f0953a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "367f44c2-5897-4d1d-80d2-72bb817f2c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2960cf72-3ad0-49f9-832c-e19e443dcf0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I decided to solve this challenge using the Azure cloud, specifically leveraging Azure Blob Storage to store the JSON file, Azure Key Vault to securely manage secrets, and Azure Databricks as the data engineering platform due to its powerful data processing capabilities. Recognizing that LATAM Airlines operates on GCP, I would adapt my approach by using Google Cloud Storage and Google Secret Manager as alternatives to Azure services. Databricks, being a multi-cloud platform compatible with Azure, AWS, and GCP, remains suitable for this solution on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2efca41-f825-442d-9004-e3481aa9dd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "874d8413-7b59-4d81-bc50-632a9d9a2908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, I install all relevant libraries on the cluster and choose to hide the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adc9a26f-37fe-427f-9182-d26fe0c795ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q emoji polars memray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f416648-9cb8-4d8a-ae07-86398a0bdbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Then I import all libraries, that I need to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5361353a-fc6a-4849-8faf-1ce7ace75f19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from src.q1_memory import q1_memory\n",
    "from src.q1_time import q1_time\n",
    "from src.q2_memory import q2_memory\n",
    "from src.q2_time import q2_time\n",
    "from src.q3_memory import q3_memory\n",
    "from src.q3_time import q3_time\n",
    "import polars as pl\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import memray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5c88ac-e769-4c2e-9d3d-61c6d3cdf41e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Then I created some variables, that I will be using later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d86e53-25e6-458d-a49e-aef21e0375ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = dbutils.secrets.get(scope=\"kv-scope\", key=\"storage-account-name\")\n",
    "container_name = \"data\"\n",
    "mount_point = \"/mnt/data\"\n",
    "file_path = \"farmers-protest-tweets-2021-2-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590f46ac-2a85-4ddd-90dd-87ae7690e1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now I could proceed and mount the Azure Blob Storage container into the Databricks environment, so I can access the JSON file directly from Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28508db-df41-49df-87a7-50c42a91443d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create this mount, if it does not already exist\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "        source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "        mount_point = mount_point,\n",
    "        extra_configs  = {f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\" : dbutils.secrets.get(scope=\"kv-scope\", key=\"access-key\")}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830ee37b-e4fa-4a89-b366-8328f2fecd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To enhance performance and efficiency, I chose to transform the JSON file into a Parquet file format. Parquet is a columnar storage format optimized for analytical queries, which improves read speed and reduces storage costs by compressing data. This choice enables faster data processing, especially for querying subsets of data, which is highly beneficial in handling large datasets like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22101c35-d8b1-41d9-a085-5d44c3662620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"JSON_to_Parquet\")\n",
    "\n",
    "def convert_json_to_parquet(input_path: str, output_path: str, overwrite: bool = True):\n",
    "    \"\"\"\n",
    "    Converts a JSON file to Parquet format using PySpark.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str\n",
    "        The path to the input JSON file.\n",
    "    output_path : str\n",
    "        The path where the Parquet file will be saved.\n",
    "    overwrite : bool, optional\n",
    "        If True, overwrites the existing Parquet file. Defaults to True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        logger.info(f\"Reading JSON file from {input_path}\")\n",
    "        json_df = spark.read.json(input_path)\n",
    "        \n",
    "        logger.info(f\"Writing DataFrame to Parquet at {output_path}\")\n",
    "        json_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        \n",
    "        logger.info(\"Conversion completed successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during JSON to Parquet conversion: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6a4c5f-7d76-47dc-89fc-76cd2bed2922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "convert_json_to_parquet(\n",
    "    input_path=f\"{mount_point}/{file_path}.json\",\n",
    "    output_path=f\"{mount_point}/{file_path}.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11f5b25-a0c9-4f43-9c3b-163e31acdb83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_file_path = f\"/dbfs{mount_point}/{file_path}.json\"\n",
    "parquet_file_path = f\"/dbfs{mount_point}/{file_path}.parquet/*.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e05c6a8-f02d-448a-b30d-855f7d1b1b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb80298-afd5-424e-96e5-6a06fca637c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Generally speaking for data manipulation and transformation, I selected Polars over PySpark and Pandas due to its performance advantages with large datasets. While Pandas is commonly used for data analysis, it’s more memory-intensive and slower with large files, making it less suitable here. PySpark, though excellent for distributed data processing, introduced initialization and processing overheads that slowed performance in this specific context. Polars, with its efficient and lightweight DataFrame operations, proved faster and more memory-efficient. Additionally, for memory-optimized functions, I opted to read the JSON file row by row, avoiding loading the entire file into memory. This row-by-row approach, combined with Polars’ efficiency, allowed me to meet both speed and memory constraints effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e57529-0ec0-4dca-98c0-37ed08f6f527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0729d4-b544-4ec4-b02a-3dfdb624bbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q1_memory(json_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ec7dca-392d-419a-ab3b-a0430396ab86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718b029c-4005-47c5-b34e-023777615caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q1_time(parquet_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68c35d8-2ec2-4dc3-9260-8aa443dbf302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q2_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d32fe1e3-5f70-41f9-b763-88b75de1ffcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q2_memory(json_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "968fcadf-e7ba-4174-b72e-3f14de2ff40f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q2_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a108d6c-79d2-40fd-8a42-87d569a9bce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q2_time(parquet_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac0b69f-3cd2-4651-a09b-0dc9838d9b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q3_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f7028c-7e1c-4e92-bb30-f428bb0f60c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q3_memory(json_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec7a3e3-908b-4a86-8b99-5faa434949ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## q3_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da6c3bb-7c20-4088-a454-b634fd3f1b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = q3_time(parquet_file_path)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592e1402-0333-41f2-a34c-c18029722c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Metrics Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa705179-f3d2-4fde-b903-7759002ff5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To provide clear insights into each function’s performance, I created a DataFrame summarizing two key metrics: peak memory usage and execution time. Each function is represented as a row, making the results easy to compare.\n",
    "\n",
    "The approach involves two separate runs for each function. In the first run, I use Memray to track memory usage and capture the peak memory consumption, ensuring we identify the highest memory demand during the function’s execution. In the second run, I measure execution time independently to avoid any potential overhead from memory profiling, resulting in more accurate timing metrics. This separation of memory and execution profiling prevents interference between metrics, allowing for a clean and reliable performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec748e8-6232-4c2f-b281-ba91d112a4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "memory_functions = [q1_memory, q2_memory, q3_memory]\n",
    "time_functions = [q1_time, q2_time, q3_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491f797d-5cfe-446e-8fb6-d64930cecdfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def measure_memory(func, *args, **kwargs):\n",
    "    # Define the output file for memray\n",
    "    output_file = f\"{func.__name__}_memray.bin\"\n",
    "    \n",
    "    # Remove the output file if it already exists to avoid conflicts\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    # Run the function with memray tracking\n",
    "    with memray.Tracker(output_file, native_traces=True):\n",
    "        func(*args, **kwargs)\n",
    "    \n",
    "    # Analyze the memray data to find peak memory usage\n",
    "    peak_memory = 0\n",
    "    reader = memray.FileReader(output_file)\n",
    "    for record in reader.get_allocation_records():\n",
    "        peak_memory = max(peak_memory, record.size)\n",
    "    \n",
    "    os.remove(output_file)\n",
    "    \n",
    "    # Convert bytes to MiB\n",
    "    peak_memory_mib = peak_memory / (1024 ** 2)\n",
    "    \n",
    "    return peak_memory_mib\n",
    "\n",
    "def measure_time(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return execution_time\n",
    "\n",
    "# Initialize list to store results\n",
    "metrics_list = []\n",
    "\n",
    "# Run profiling for memory-based functions (using JSON file)\n",
    "for func in memory_functions:\n",
    "    avg_memory = measure_memory(func, json_file_path)  # Measure memory usage with JSON file\n",
    "    execution_time = measure_time(func, json_file_path)  # Measure execution time with JSON file\n",
    "    metrics_list.append({\n",
    "        \"Function Name\": func.__name__,\n",
    "        \"Peak Memory Usage (MiB)\": round(avg_memory, 2),\n",
    "        \"Execution Time (seconds)\": round(execution_time, 2)\n",
    "    })\n",
    "\n",
    "# Run profiling for time-based functions (using Parquet file)\n",
    "for func in time_functions:\n",
    "    avg_memory = measure_memory(func, parquet_file_path)  # Measure memory usage with Parquet file\n",
    "    execution_time = measure_time(func, parquet_file_path)  # Measure execution time with Parquet file\n",
    "    metrics_list.append({\n",
    "        \"Function Name\": func.__name__,\n",
    "        \"Peak Memory Usage (MiB)\": round(avg_memory, 2),\n",
    "        \"Execution Time (seconds)\": round(execution_time, 2)\n",
    "    })\n",
    "\n",
    "# Convert the list of metrics to a Polars DataFrame\n",
    "metrics_df = pl.DataFrame(metrics_list)\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "challenge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
